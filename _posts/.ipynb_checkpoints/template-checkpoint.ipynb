{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post             \n",
    "comments: true           \n",
    "title: 'Methods to encode categorical variables'           \n",
    "excerpt: 'Feature Engineering: Part 1'            \n",
    "date: 2019-05-05 04:00:00      \n",
    "mathjax: false       \n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://pbpython.com/categorical-encoding.html\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Encoding categorical variables is an important step in the data science process. Because there are multiple approaches to encoding variables, it is important to understand the various options and how to implement them on your own data sets. The python data science ecosystem has many helpful approaches to handling these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to encode categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert to number:              \n",
    "    Some ML libraries do not take categorical variables as input. Thus we convert them in to numerical variables. Below are the methods to convert a categorical string variable input to numerical nature:              \n",
    "###     1.1 Numerical Encoding: \n",
    "        - Numerical Encoding is very simple: assign an arbitrary number to each category.\n",
    "        - It is used to transform non-numerical labels to numerical labels (or nominal categorical varaibles). Numerical labels are always between 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprosessing import LabelEncoder\n",
    "\n",
    "number= LabelEncoder()\n",
    "train['sex'] = number.fit_transform(train['sex'].astype('str'))\n",
    "test['sex'] = number.fit_transform(test['sex'].astype('str'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common challenge with nominal categorical variable is that, it may decrease performance of a model. \n",
    "    - For example: We have two features “age” (range: 0-80) and “city” (81 different levels). Now, when we’ll apply label encoder to ‘city’ variable, it will represent ‘city’ with numeric values range from 0 to 80. The ‘city’ variable is now similar to ‘age’ variable since both will have similar data points, which is certainly not a right approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert numeric bins to number: \n",
    "        - It is used to transform non-numerical labels to numerical labels (or nominal categorical varaibles). Numerical labels are always between 0 and n_classes-1.\n",
    "    \n",
    "### 1.3 Combine Levels\n",
    "        - To avoid redundant levels in a categorical variable and to deal with rare levels, we can simply combine the different levels.\n",
    "        1.3.1 Using Business Logic\n",
    "        1.3.2 Using Frequency or Response Rate\n",
    "        \n",
    "### 1.4 Dummy Coding\n",
    "        - Dummy coding is a commonly used method for converting a categorical input variable into continuous variable. Presence of a level is represented by 1 and absence is represented by 0. For every level present, one dummy variable is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 One Hot Encoding:\n",
    "    - Label encoding has the advantage that it is straightforward, but it has the disadvantage that the numeric values can be misinterpreted by the algorithms. \n",
    "    - A common alternative approach is called one hot encoding (but also goes by several different names shown below). Despite the different names, the basic strategy is to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.\n",
    "    - This function is powerful because you can pass as many category columns as you would like and choose how to label the columns using prefix . Proper naming will make the rest of the analysis just a little bit easier.\n",
    "    - One hot encoding is very useful but it can cause the number of columns to expand greatly if you have very many unique values in a column. \n",
    "    - In addition to thinking about what One-Hot Encoding does, you will notice something very quickly:\n",
    "        - You have as many columns as you have cardinalities (values) in the categorical variable.\n",
    "        - You have a bunch of zeroes and only few 1s! (one 1 per new feature)              \n",
    "        \n",
    "Therefore, you have to choose between two representations of One-Hot Encoding:\n",
    "        - Dense Representation: 0s are stored in memory, which ballons the RAM usage a LOT if you have many cardinalities. But at least, the support for such representation is typicallY worldwide.\n",
    "        - Sparse Repsentation: 0s are not stored in memory, which makes RAM efficiency a LOT better even if you have millions of cardinalities. However, good luck finding support for sparse matrices for machine learning, because it is not widespread (think: xgboost, LightGBM, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Custom Binary Encoding\n",
    "    - Depending on the data set, you may be able to use some combination of label encoding and one hot encoding to create a binary column that meets your needs for further analysis.\n",
    "    - This approach can be really useful if there is an option to consolidate to a simple Y/N value in a column. This also highlights how important domain knowledge is to solving the problem in the most efficient manner possible.\n",
    "    - Use power law of binary encoding to store **N** cardinalities using ceil(log(N+1)/log(2)) features.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb_style = LabelBinarizer()\n",
    "lb_results = lb_style.fit_transform(obj_df[\"body_style\"])\n",
    "pd.DataFrame(lb_results, columns=lb_style.classes_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Backward Difference Encoding\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Polynomial Encoding\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to deal with high cardinality\n",
    "- Run chi-squared tests or odds-ratios for the categorical variable and dependent variable to reduce the number of categories.\n",
    "- Use algorithms like Random Forests or Lasso to get feature importances on all the one-hot encoded columns and only keep those that have a certain level of feature importance.\n",
    "- Perform Mean Encoding based on training set to set the mean value of that categorical variable in that column. For example, if 30% of people with brown hair are 1's, that column becomes .3 for anyone with brown hair. The issue with this is that if interpretability is important you generally lose that, as a column that indicates a poor neighborhood having a relative feature importance of .30 doesn't mean the same thing as categories that have an expected value of an arbitrary value has a feature importance of .3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
